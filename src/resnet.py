import os
import sys
import datetime
import numpy as np
import tensorflow as tf
import tensorflow.keras.models as models
import tensorflow.keras.layers as layers
import tensorflow.keras.callbacks as callbacks
import tensorflow.keras.optimizers as optimizers
import tensorflow.keras.regularizers as regularizers
import tensorflow.keras.initializers as initializers
import tensorflow.keras.preprocessing.image as kerasimage
from .softmax import learning_rate_schedule
from .get_data import data

# uncomment to disable GPU support
# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

default_path = 'C:/Users/DimKa/Documents/'


def resnet(image_shape, num_layers=56):

    if (num_layers - 2) % 6 != 0:
        raise ValueError('n_layers should be 6n+2 (eg 20, 32, 44, 56)')
    n = (num_layers - 2) // 6

    inputs = layers.Input(shape=image_shape)

    # First layer
    x = layers.Conv2D(16, 3, use_bias=False, kernel_regularizer=regularizers.l2(1e-4),
                      padding='same', kernel_initializer='he_uniform')(inputs)
    x = layers.BatchNormalization(scale=False)(x)
    x = layers.Activation('relu')(x)

    # Stack blocks
    for i_block in range(n):
        x = resnet_block(x, 16, strides=1)

    for i_block in range(n):
        x = resnet_block(x, 32, strides=2 if i_block == 0 else 1)

    for i_block in range(n):
        x = resnet_block(x, 64, strides=2 if i_block == 0 else 1)

    # Global pooling and classifier on top
    x = layers.GlobalAveragePooling2D()(x)
    outputs = layers.Dense(
        10, kernel_regularizer=regularizers.l2(1e-4))(x)

    return models.Model(inputs=inputs, outputs=outputs, name=f'notresnet{num_layers}')


def resnet_block(x, n_channels_out, strides=1):
    # First conv
    f = layers.Conv2D(n_channels_out, 3, strides, use_bias=False,
                      kernel_regularizer=regularizers.l2(1e-4),
                      padding='same', kernel_initializer='he_uniform')(x)
    f = layers.BatchNormalization(scale=False)(f)
    f = layers.Activation('relu')(f)

    # Second conv
    f = layers.Conv2D(n_channels_out, 3, use_bias=False,
                      kernel_regularizer=regularizers.l2(1e-4),
                      padding='same', kernel_initializer='he_uniform')(f)
    f = layers.BatchNormalization(scale=False)(f)

    # The shortcut connection is just the identity.
    # If feature channel counts differ between input and output,
    # zero padding is used to match the depths.
    # This is implemented by a Conv2D with fixed weights.
    n_channels_in = x.shape[-1]
    if n_channels_in != n_channels_out:
        # Fixed weights, np.eye returns a matrix with 1s along the
        # main diagonal and zeros elsewhere.
        identity_weights = np.eye(n_channels_in, n_channels_out, dtype=np.float32)
        layer = layers.Conv2D(
            n_channels_out, kernel_size=1, strides=strides, use_bias=False,
            kernel_initializer=initializers.Constant(value=identity_weights))
        # Not learned! Set trainable to False:
        layer.trainable = False
        x = layer(x)

    # This is where the ResNet magic happens: the shortcut connection is
    # added to the residual.
    x = layers.add([x, f])
    return layers.Activation('relu')(x)


class ResNet(object):

    def __init__(self, path=default_path):
        self.x_train, self.y_train, self.x_test, self.y_test, self.image_shape, self.log_root, self.labels = data(path)

    def train_with_lr_decay(self, model, augment=False):
        model.compile(loss=tf.compat.v2.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                      metrics=['accuracy'], optimizer=optimizers.Adam(lr=1e-3))

        # Callback for learning rate adjustment (see below)
        lr_scheduler = callbacks.LearningRateScheduler(learning_rate_schedule)

        # TensorBoard callback
        timestamp = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')
        logdir = os.path.join(self.log_root, f'{model.name}_{timestamp}')
        tensorboard_callback = callbacks.TensorBoard(logdir, profile_batch=0)

        if augment:
            # Data augmentation: flip and shift horizontally/vertically by max 4 pixels
            datagen = kerasimage.ImageDataGenerator(
                width_shift_range=4, height_shift_range=4,
                horizontal_flip=True, fill_mode='constant')

            # Fit the model on the batches generated by datagen.flow()
            model.fit_generator(
                datagen.flow(self.x_train, self.y_train, batch_size=128), validation_data=(self.x_test, self.y_test),
                epochs=150, verbose=1, callbacks=[lr_scheduler, tensorboard_callback])
        else:
            # Fit the model on the batches generated by datagen.flow()
            model.fit(self.x_train, self.y_train, batch_size=128, validation_data=(self.x_test, self.y_test),
                      epochs=150, verbose=1, callbacks=[lr_scheduler, tensorboard_callback])

    def main(self):
        if 2 <= len(sys.argv) < 3:
            if sys.argv[1] == '-a':
                self.train_with_lr_decay(resnet(self.image_shape), augment=True)
            else:
                self.train_with_lr_decay(resnet(self.image_shape), augment=False)
        else:
            self.train_with_lr_decay(resnet(self.image_shape), augment=False)

if __name__ == "__main__":
    Network = ResNet()
    Network.main()
